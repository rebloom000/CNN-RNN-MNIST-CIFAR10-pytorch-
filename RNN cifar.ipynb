{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c8030b9-b069-400e-8a19-b6ae35cb1734",
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入包\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data.dataloader import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c41eec24-6187-455b-a1e8-3cad6f4d3989",
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建 transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,),(0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1b2d23c-18b9-401f-8321-105639581d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/myconda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 3, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    }
   ],
   "source": [
    "#下载 加载数据集\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./CIFAR10', train=True, download=True, transform=transform)\n",
    "testset = datasets.CIFAR10(root='./CIFAR10', train=False, download=True, transform=transform)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "device = \"cuda\"\n",
    "\n",
    "train_loader = DataLoader(trainset,batch_size = BATCH_SIZE, shuffle = True, num_workers = 16, pin_memory = True)\n",
    "test_loader = DataLoader(testset,batch_size = BATCH_SIZE, shuffle = True, num_workers = 16, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0df5959-4ba0-4902-acad-c8f0a8e56ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义网络模型\n",
    "\n",
    "class RNN_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN_Net,self).__init__()\n",
    "        self.hidden_dim = 128\n",
    "        self.layer_dim = 3\n",
    "        self.rnn = nn.RNN(32*3, 128, 3, batch_first=True, nonlinearity='relu')\n",
    "        self.fc = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # （layer_dim, batch_size, hidden_dim)\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_().to(device)\n",
    "        # 分离隐藏状态，避免梯度爆炸\n",
    "        out, hn = self.rnn(x, h0.detach().cuda())\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "# 创建模型\n",
    "\n",
    "net = RNN_Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e165026d-26e0-4188-8624-14c45f5b2ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义优化器和损失函数\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # 交叉式损失函数\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) # 优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690ebb8f-7725-42fd-9df4-b1e62c60ca9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :1 , Loss : 0.018\n",
      "Epoch :2 , Loss : 0.018\n",
      "Epoch :3 , Loss : 0.018\n",
      "Epoch :4 , Loss : 0.017\n",
      "Epoch :5 , Loss : 0.017\n",
      "Epoch :6 , Loss : 0.016\n",
      "Epoch :7 , Loss : 0.015\n",
      "Epoch :8 , Loss : 0.015\n",
      "Epoch :9 , Loss : 0.014\n",
      "Epoch :10 , Loss : 0.014\n",
      "Epoch :11 , Loss : 0.014\n",
      "Epoch :12 , Loss : 0.013\n",
      "Epoch :13 , Loss : 0.013\n",
      "Epoch :14 , Loss : 0.013\n",
      "Epoch :15 , Loss : 0.013\n",
      "Epoch :16 , Loss : 0.012\n",
      "Epoch :17 , Loss : 0.012\n",
      "Epoch :18 , Loss : 0.012\n",
      "Epoch :19 , Loss : 0.012\n",
      "Epoch :20 , Loss : 0.012\n",
      "Epoch :21 , Loss : 0.012\n",
      "Epoch :22 , Loss : 0.012\n",
      "Epoch :23 , Loss : 0.011\n",
      "Epoch :24 , Loss : 0.011\n",
      "Epoch :25 , Loss : 0.011\n",
      "Epoch :26 , Loss : 0.011\n",
      "Epoch :27 , Loss : 0.011\n",
      "Epoch :28 , Loss : 0.011\n",
      "Epoch :29 , Loss : 0.011\n",
      "Epoch :30 , Loss : 0.011\n",
      "Epoch :31 , Loss : 0.010\n",
      "Epoch :32 , Loss : 0.010\n",
      "Epoch :33 , Loss : 0.010\n",
      "Epoch :34 , Loss : 0.010\n",
      "Epoch :43 , Loss : 0.010\n",
      "Epoch :44 , Loss : 0.009\n",
      "Epoch :45 , Loss : 0.009\n",
      "Epoch :46 , Loss : 0.009\n",
      "Epoch :47 , Loss : 0.009\n",
      "Epoch :48 , Loss : 0.009\n",
      "Epoch :49 , Loss : 0.009\n",
      "Epoch :50 , Loss : 0.009\n",
      "Epoch :51 , Loss : 0.009\n",
      "Epoch :52 , Loss : 0.009\n",
      "Epoch :53 , Loss : 0.009\n",
      "Epoch :54 , Loss : 0.009\n",
      "Epoch :55 , Loss : 0.009\n",
      "Epoch :56 , Loss : 0.009\n",
      "Epoch :57 , Loss : 0.009\n",
      "Epoch :58 , Loss : 0.009\n",
      "Epoch :59 , Loss : 0.008\n",
      "Epoch :60 , Loss : 0.008\n",
      "Epoch :61 , Loss : 0.008\n",
      "Epoch :62 , Loss : 0.008\n",
      "Epoch :63 , Loss : 0.008\n",
      "Epoch :64 , Loss : 0.008\n",
      "Epoch :65 , Loss : 0.008\n",
      "Epoch :66 , Loss : 0.008\n",
      "Epoch :67 , Loss : 0.008\n",
      "Epoch :68 , Loss : 0.008\n",
      "Epoch :69 , Loss : 0.008\n",
      "Epoch :70 , Loss : 0.008\n",
      "Epoch :71 , Loss : 0.008\n",
      "Epoch :72 , Loss : 0.008\n",
      "Epoch :73 , Loss : 0.008\n",
      "Epoch :74 , Loss : 0.008\n",
      "Epoch :75 , Loss : 0.008\n",
      "Epoch :76 , Loss : 0.007\n",
      "Epoch :77 , Loss : 0.007\n",
      "Epoch :78 , Loss : 0.007\n",
      "Epoch :79 , Loss : 0.007\n",
      "Epoch :80 , Loss : 0.007\n",
      "Epoch :81 , Loss : 0.007\n",
      "Epoch :82 , Loss : 0.007\n",
      "Epoch :83 , Loss : 0.007\n",
      "Epoch :84 , Loss : 0.007\n",
      "Epoch :85 , Loss : 0.007\n",
      "Epoch :86 , Loss : 0.007\n",
      "Epoch :87 , Loss : 0.007\n",
      "Epoch :88 , Loss : 0.007\n",
      "Epoch :89 , Loss : 0.007\n",
      "Epoch :90 , Loss : 0.007\n",
      "Epoch :91 , Loss : 0.007\n",
      "Epoch :92 , Loss : 0.007\n",
      "Epoch :93 , Loss : 0.007\n",
      "Epoch :94 , Loss : 0.007\n",
      "Epoch :95 , Loss : 0.006\n",
      "Epoch :96 , Loss : 0.007\n",
      "Epoch :97 , Loss : 0.006\n",
      "Epoch :98 , Loss : 0.006\n",
      "Epoch :99 , Loss : 0.006\n",
      "Epoch :100 , Loss : 0.006\n",
      "Epoch :101 , Loss : 0.006\n",
      "Epoch :102 , Loss : 0.006\n",
      "Epoch :103 , Loss : 0.006\n",
      "Epoch :104 , Loss : 0.006\n",
      "Epoch :105 , Loss : 0.006\n",
      "Epoch :106 , Loss : 0.006\n",
      "Epoch :107 , Loss : 0.006\n",
      "Epoch :108 , Loss : 0.006\n",
      "Epoch :109 , Loss : 0.006\n",
      "Epoch :110 , Loss : 0.006\n",
      "Epoch :111 , Loss : 0.006\n",
      "Epoch :112 , Loss : 0.006\n",
      "Epoch :113 , Loss : 0.006\n",
      "Epoch :114 , Loss : 0.006\n",
      "Epoch :115 , Loss : 0.006\n",
      "Epoch :116 , Loss : 0.006\n",
      "Epoch :117 , Loss : 0.005\n",
      "Epoch :118 , Loss : 0.005\n",
      "Epoch :119 , Loss : 0.005\n",
      "Epoch :120 , Loss : 0.005\n",
      "Epoch :121 , Loss : 0.005\n",
      "Epoch :122 , Loss : 0.005\n",
      "Epoch :123 , Loss : 0.005\n",
      "Epoch :135 , Loss : 0.005\n",
      "Epoch :136 , Loss : 0.005\n",
      "Epoch :137 , Loss : 0.005\n",
      "Epoch :138 , Loss : 0.005\n",
      "Epoch :139 , Loss : 0.005\n",
      "Epoch :140 , Loss : 0.005\n",
      "Epoch :141 , Loss : 0.004\n",
      "Epoch :142 , Loss : 0.005\n",
      "Epoch :143 , Loss : 0.004\n",
      "Epoch :144 , Loss : 0.004\n",
      "Epoch :145 , Loss : 0.004\n",
      "Epoch :146 , Loss : 0.004\n",
      "Epoch :147 , Loss : 0.004\n",
      "Epoch :148 , Loss : 0.004\n",
      "Epoch :149 , Loss : 0.004\n",
      "Epoch :150 , Loss : 0.004\n",
      "Epoch :151 , Loss : 0.004\n",
      "Epoch :152 , Loss : 0.004\n",
      "Epoch :153 , Loss : 0.004\n",
      "Epoch :154 , Loss : 0.004\n",
      "Epoch :155 , Loss : 0.004\n",
      "Epoch :156 , Loss : 0.004\n",
      "Epoch :157 , Loss : 0.004\n",
      "Epoch :158 , Loss : 0.004\n",
      "Epoch :159 , Loss : 0.004\n",
      "Epoch :160 , Loss : 0.004\n",
      "Epoch :161 , Loss : 0.004\n",
      "Epoch :162 , Loss : 0.004\n",
      "Epoch :163 , Loss : 0.004\n",
      "Epoch :164 , Loss : 0.004\n",
      "Epoch :165 , Loss : 0.004\n",
      "Epoch :166 , Loss : 0.004\n",
      "Epoch :167 , Loss : 0.004\n",
      "Epoch :168 , Loss : 0.004\n",
      "Epoch :169 , Loss : 0.004\n",
      "Epoch :170 , Loss : 0.003\n",
      "Epoch :171 , Loss : 0.004\n",
      "Epoch :172 , Loss : 0.004\n",
      "Epoch :173 , Loss : 0.003\n",
      "Epoch :174 , Loss : 0.004\n",
      "Epoch :175 , Loss : 0.003\n",
      "Epoch :176 , Loss : 0.003\n",
      "Epoch :177 , Loss : 0.003\n",
      "Epoch :178 , Loss : 0.003\n",
      "Epoch :179 , Loss : 0.003\n",
      "Epoch :180 , Loss : 0.003\n",
      "Epoch :181 , Loss : 0.003\n",
      "Epoch :182 , Loss : 0.003\n",
      "Epoch :183 , Loss : 0.003\n",
      "Epoch :184 , Loss : 0.003\n",
      "Epoch :185 , Loss : 0.003\n",
      "Epoch :186 , Loss : 0.003\n",
      "Epoch :187 , Loss : 0.003\n",
      "Epoch :188 , Loss : 0.003\n",
      "Epoch :189 , Loss : 0.003\n",
      "Epoch :190 , Loss : 0.003\n",
      "Epoch :191 , Loss : 0.003\n",
      "Epoch :192 , Loss : 0.003\n",
      "Epoch :193 , Loss : 0.003\n",
      "Epoch :194 , Loss : 0.003\n",
      "Epoch :195 , Loss : 0.003\n",
      "Epoch :196 , Loss : 0.003\n",
      "Epoch :197 , Loss : 0.003\n",
      "Epoch :198 , Loss : 0.003\n"
     ]
    }
   ],
   "source": [
    "#训练\n",
    "\n",
    "EPOCHS = 200\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for i, (datas, labels) in enumerate(train_loader):\n",
    "        # 一个batch的数据转换为RNN的输入维度\n",
    "        #32*32*3\n",
    "        #数据处理\n",
    "        datas = datas.view(-1, 32, 32*3).requires_grad_().to(device) \n",
    "        labels = labels.to(device)\n",
    "        # 梯度置零\n",
    "        optimizer.zero_grad()\n",
    "        # 训练\n",
    "        outputs = net(datas)\n",
    "        # 计算损失\n",
    "        loss = criterion(outputs, labels)\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        # 参数更新\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    print(\"Epoch :%d , Loss : %.3f\"%(epoch+1, train_loss/len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fd9fef-eac5-405a-8510-79300b5042c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for i , (datas, labels) in enumerate(test_loader):\n",
    "        datas = datas.view(-1, sequence_dim, input_dim).to(device)\n",
    "        outputs = net(datas)\n",
    "        _, predicted = torch.max(outputs.data, dim=1) # 第一个是值的张量，第二个是序号的张量\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted.cuda() == labels.cuda()).sum()\n",
    "    print(\"Total Accuracy：{:.3f}%\".format(correct / total * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d623d5-1f48-4678-81c6-236c6792c655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 显示每一类预测的概率\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "class_correct = list(0. for i in range(10))\n",
    "total = list(0. for i in range(10))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (images, labels) in test_loader:\n",
    "        images = images.view(-1, 32, 32*3).to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, dim=1) # 获取到每一行最大值的索引\n",
    "        c = (predicted.cuda() == labels.cuda()).squeeze() # squeeze() 去掉0维【默认】， unsqueeze() 增加一维\n",
    "        if labels.shape[0] == 128:\n",
    "            for i in range(BATCH_SIZE):\n",
    "                label = labels[i] # 获取每一个label\n",
    "                class_correct[label] += c[i].item() # 累计为True的个数, 注意：1 + True = 2, 1 + False = 1\n",
    "                total[label] += 1 # 该类总的个数\n",
    "            \n",
    "# 输出正确率\n",
    "for i in range(10):\n",
    "    print(\"Accuracy ： %5s : %2d %%\" % (classes[i], 100 * class_correct[i] / total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33577101-52a7-48d6-a7ba-f6226ee21811",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "myconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
